{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`xt = tx1 + (1 − t)x0 `              \n",
    "\n",
    " This equation represents a linear interpolation between two points x0 and x1 using a parameter t.\n",
    "\n",
    "The variable xt is the interpolated point between x0 and x1. t is the interpolation parameter that ranges from 0 to 1.\n",
    "\n",
    "When t=0, xt = x0, meaning xt is equal to the first point x0. \n",
    "\n",
    "When t=1, xt = x1, meaning xt is equal to the second point x1.\n",
    "\n",
    "For values of t between 0 and 1, xt is a linear combination of x0 and x1. Specifically, xt is equal to x0 multiplied by (1 - t) plus x1 multiplied by t. \n",
    "\n",
    "As t increases from 0 to 1, the interpolated point xt moves linearly from x0 to x1 along the line segment between those two points. This allows you to find intermediate points along a straight line path between x0 and x1.\n",
    "\n",
    "In summary, this equation performs linear interpolation between two endpoints based on a parameter t. By varying t from 0 to 1, xt moves uniformly along the straight line path between x0 and x1.\n",
    "\n",
    "\n",
    "`xt ∼ N (tx1, (1 − t)2I),`\n",
    "\n",
    " This notation represents a normal (Gaussian) distribution with the following properties:\n",
    "\n",
    "- xt is a random vector of variables that is normally distributed.\n",
    "- The mean of xt is tx1, where t is a scalar and x1 is a fixed vector. So the mean scales linearly with t.\n",
    "- The covariance matrix is (1 - t)2I, where I is the identity matrix. So the covariance scales quadratically with (1 - t). \n",
    "- As t increases, the variance decreases quadratically towards 0. When t=1, the variance is 0 and xt collapses to the fixed vector x1.\n",
    "- As t decreases towards 0, the variance increases quadratically, spreading out the distribution.\n",
    "\n",
    "In summary, xt interpolates between a fixed vector x1 and a normal distribution with increasing variance as t goes from 1 to 0. The parameter t controls how close xt sticks to the fixed mean x1 versus being spread out.\n",
    "\n",
    "\n",
    "`xt = tx1 ⊕ (1 − t) Down(x0, 2K ),`\n",
    "\n",
    " This equation is calculating a new image xt by combining two existing images x0 and x1 using a weighted average.\n",
    "\n",
    "Specifically:\n",
    "\n",
    "- tx1 represents image x1 weighted by a factor t. So if t=0.5, this would be 0.5 * x1.\n",
    "\n",
    "- (1 - t) Down(x0, 2K) represents image x0 downscaled by a factor of 2K (reducing resolution), weighted by (1 - t). So if t=0.5, this would be 0.5 * Down(x0, 2K). \n",
    "\n",
    "- The ⊕ symbol represents element-wise addition of the two weighted images. \n",
    "\n",
    "So in summary, xt is computed by taking a weighted average of x1 and a downscaled version of x0, controlled by the weighting factor t. This has the effect of transitioning smoothly from x0 to x1 as t goes from 0 to 1, while matching resolutions by downscaling x0. The downscaling of x0 avoids artifacts when combining images of different sizes.\n",
    "\n",
    "\n",
    "`xt = t′ Down(xek , 2k) + (1 − t′) Up(Down(xsk , 2k+1)),`\n",
    "\n",
    "\n",
    " This equation defines xt as a weighted combination of two downsampled images xek and xsk.\n",
    "\n",
    "Specifically:\n",
    "- xek is downsampled by a factor of 2k \n",
    "- xsk is downsampled by a factor of 2k+1 and then upsampled back to the original size\n",
    "- t' is a weighting factor between 0 and 1\n",
    "- xt is computed as:\n",
    "    - t' * Downsampled xek \n",
    "    + (1 - t') * Upsampled downsampled xsk\n",
    "\n",
    "So xt is a mixture of a mildly and heavily downsampled version of the input images, with relative weighting determined by t'. This has the effect of producing a smoothed/blurred combination of the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), dilation=(2, 2))\n",
      "tensor([[[[-2.8445e+00,  5.1145e-01, -1.2006e+00, -1.6100e+00,  5.1081e-01,\n",
      "           -1.4714e+00, -8.5396e-01,  4.5243e-01,  1.2125e+00,  1.1608e+00],\n",
      "          [-5.6500e-01,  7.6027e-01, -8.7526e-01, -1.3108e-02,  2.3209e-01,\n",
      "            8.2394e-01,  3.0478e-01,  1.1288e+00,  7.6737e-01,  1.7017e+00],\n",
      "          [-2.1264e-01, -1.8750e+00, -2.1112e+00,  2.4271e-01, -5.3580e-01,\n",
      "            5.8242e-01, -9.6039e-01,  2.9491e-01,  4.2169e-01, -2.5280e-01],\n",
      "          [-3.3099e-01, -7.0068e-01, -2.2981e-01,  1.5504e+00, -4.4323e-01,\n",
      "            1.3953e+00, -2.0092e+00,  1.4159e-01,  1.7603e+00,  1.3945e+00],\n",
      "          [ 6.9160e-01,  3.9457e-01, -6.2283e-01, -9.3275e-01,  4.1003e-01,\n",
      "           -2.0210e+00,  9.8246e-01,  2.0212e-01,  6.6417e-01,  1.0909e+00],\n",
      "          [ 2.9392e-01, -5.8461e-01,  5.7005e-01,  1.6243e-02, -9.4424e-01,\n",
      "           -6.2767e-03,  1.1252e+00, -1.2797e+00, -4.2013e-04,  9.2083e-01],\n",
      "          [ 8.4634e-01, -1.7510e+00,  1.5702e+00,  1.6823e+00, -2.1868e-01,\n",
      "            4.4612e-01,  4.7490e-01, -1.4383e+00,  1.2813e+00, -3.7939e-01],\n",
      "          [-9.2285e-01,  2.2049e-01, -1.3121e+00,  5.8053e-01, -1.4704e+00,\n",
      "           -2.8910e-01, -1.2073e+00,  2.0834e+00, -1.5668e+00,  2.3620e-01],\n",
      "          [ 4.6905e-01, -8.1776e-02, -2.3895e+00, -2.6361e+00, -1.2031e+00,\n",
      "           -1.5069e+00, -1.9449e-01, -6.4313e-01,  2.8830e-01, -1.1075e+00],\n",
      "          [-5.6273e-01,  5.7632e-01,  6.6033e-01, -2.9893e+00,  3.3569e-01,\n",
      "           -1.0924e+00, -2.7458e-01, -1.6318e-01,  3.2703e-01, -2.4374e+00]]]])\n",
      "torch.Size([1, 1, 6, 6])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a 2D convolutional layer with dilation\n",
    "conv_layer = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, dilation=2)\n",
    "print(conv_layer)\n",
    "\n",
    "# Input tensor of shape (batch_size, channels, height, width)\n",
    "input_tensor = torch.randn(1, 1, 10, 10)\n",
    "print(input_tensor)\n",
    "\n",
    "# Apply the convolutional layer\n",
    "output_tensor = conv_layer(input_tensor)\n",
    "\n",
    "print(output_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "from typing import Tuple, Union\n",
    "\n",
    "\n",
    "class CausalConv3d(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel_size: int = 3,\n",
    "        stride: Union[int, Tuple[int]] = 1,\n",
    "        dilation: int = 1,\n",
    "        groups: int = 1,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        kernel_size = (kernel_size, kernel_size, kernel_size)\n",
    "        self.time_kernel_size = kernel_size[0]\n",
    "\n",
    "        dilation = (dilation, 1, 1)\n",
    "\n",
    "        height_pad = kernel_size[1] // 2\n",
    "        width_pad = kernel_size[2] // 2\n",
    "        padding = (0, height_pad, width_pad)\n",
    "\n",
    "        self.conv = nn.Conv3d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            stride=stride,\n",
    "            dilation=dilation,\n",
    "            padding=padding,\n",
    "            padding_mode=\"zeros\",\n",
    "            groups=groups,\n",
    "        )\n",
    "\n",
    "    def forward(self, x, causal: bool = True):\n",
    "        print(\"x\", x.shape)  # -> ([2, 3, 10, 20, 20]) -> (Batch_size, channels, depth, height, width)\n",
    "        print(\"x details: \", x[:, :, :1, :, :].shape)  # ([2, 3, 1, 20, 20])\n",
    "        \n",
    "        if causal:\n",
    "            first_frame_pad = x[:, :, :1, :, :].repeat(\n",
    "                (1, 1, self.time_kernel_size - 1, 1, 1)   \n",
    "            )\n",
    "            print(\"first frame padding\", first_frame_pad.shape) # ([2, 3, 2, 20, 20])\n",
    "            \n",
    "            detail_first_frame_pad = x[:, :, :1, :, :]  # ([2, 3, 1, 20, 20])\n",
    "            detail_first_frame_pad = detail_first_frame_pad.repeat(1, 1, self.time_kernel_size - 1, 1, 1)  #  kernel_size -> 3 - 1 = 2 \n",
    "            print(\"detail first frame padding: \", detail_first_frame_pad.shape) # ([2, 3, 2, 20, 20])\n",
    "            \n",
    "            x = torch.cat((first_frame_pad, x), dim=2)\n",
    "            print(\"x\", x.shape)  # ([2, 3, 12, 20, 20]), ([2, 3, 10, 20, 20])\n",
    "            \n",
    "            print(\"first frame padding in x: \", x[0].shape) # ([3, 12, 20, 20])\n",
    "            print(\"first frame padding in x1: \", x[1].shape)\n",
    "           \n",
    "            \n",
    "        else:\n",
    "            first_frame_pad = x[:, :, :1, :, :].repeat(\n",
    "                (1, 1, (self.time_kernel_size - 1) // 2, 1, 1)\n",
    "            )\n",
    "            print(f\"first frame padding: {first_frame_pad.shape}\")   # ([2, 3, 1, 20, 20])\n",
    "            \n",
    "            last_frame_pad = x[:, :, -1:, :, :].repeat(\n",
    "                (1, 1, (self.time_kernel_size - 1) // 2, 1, 1)\n",
    "            )\n",
    "            \n",
    "            print(\"last frame padding\", last_frame_pad.shape)\n",
    "            \n",
    "            \n",
    "            x = torch.cat((first_frame_pad, x, last_frame_pad), dim=2)\n",
    "            print(\"x\", x.shape)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "    @property\n",
    "    def weight(self):\n",
    "        return self.conv.weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input Tensor:  torch.Size([2, 3, 10, 20, 20])\n",
      "causal conv3d layer:  CausalConv3d(\n",
      "  (conv): Conv3d(3, 6, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))\n",
      ")\n",
      "x torch.Size([2, 3, 10, 20, 20])\n",
      "x details:  torch.Size([2, 3, 1, 20, 20])\n",
      "first frame padding torch.Size([2, 3, 2, 20, 20])\n",
      "detail first frame padding:  torch.Size([2, 3, 2, 20, 20])\n",
      "x torch.Size([2, 3, 12, 20, 20])\n",
      "first frame padding in x:  torch.Size([3, 12, 20, 20])\n",
      "first frame padding in x1:  torch.Size([3, 12, 20, 20])\n",
      "x torch.Size([2, 3, 10, 20, 20])\n",
      "x details:  torch.Size([2, 3, 1, 20, 20])\n",
      "first frame padding: torch.Size([2, 3, 1, 20, 20])\n",
      "last frame padding torch.Size([2, 3, 1, 20, 20])\n",
      "x torch.Size([2, 3, 12, 20, 20])\n",
      "Output Shape with Causal Padding: torch.Size([2, 6, 10, 20, 20])\n",
      "Output Shape with Non-Causal Padding: torch.Size([2, 6, 10, 20, 20])\n"
     ]
    }
   ],
   "source": [
    "# Sample Input Tensor\n",
    "batch_size = 2\n",
    "channels = 3\n",
    "depth = 10  # Time dimension\n",
    "height = 20\n",
    "width = 20\n",
    "\n",
    "# Random tensor with the shape (batch_size, channels, depth, height, width)\n",
    "input_tensor = torch.randn(batch_size, channels, depth, height, width)\n",
    "print(\"input Tensor: \", input_tensor.shape)\n",
    "\n",
    "# Instantiate the CausalConv3d layer\n",
    "causal_conv3d_layer = CausalConv3d(in_channels=channels, out_channels=6, kernel_size=3)\n",
    "print(\"causal conv3d layer: \", causal_conv3d_layer)\n",
    "\n",
    "# Apply the causal convolution with causal=True\n",
    "output_tensor_causal = causal_conv3d_layer(input_tensor, causal=True)\n",
    "\n",
    "# Apply the causal convolution with causal=False\n",
    "output_tensor_non_causal = causal_conv3d_layer(input_tensor, causal=False)\n",
    "\n",
    "# Print the shapes of the output tensors\n",
    "print(\"Output Shape with Causal Padding:\", output_tensor_causal.shape)\n",
    "print(\"Output Shape with Non-Causal Padding:\", output_tensor_non_causal.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# formula of dialation: R + (R - 1) x (d - 1)\n",
    "# R - currnel size \n",
    "# d - dilation \n",
    "\n",
    "3 + ( 3 - 1)* (2 - 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.nn.init.kaiming_uniform_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight: tensor([[7.9278e+28, 9.6269e-43, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]])\n",
      "tensor([[ 0.3333,  0.1480, -0.4159, -0.4272,  0.2004],\n",
      "        [-0.2514,  0.2156,  0.3105, -0.3438,  0.2856],\n",
      "        [-0.3329, -0.2017, -0.0799, -0.3891, -0.3182]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "weight = torch.empty(3, 5)\n",
    "print(f\"Weight: {weight}\")\n",
    "nn.init.kaiming_uniform_(weight, a=math.sqrt(5))\n",
    "print(weight)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.init._calculate_fan_in_and_fan_out(self.weight1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight tensor([[[[[0., 0., 0.],\n",
      "           [0., 0., 0.],\n",
      "           [0., 0., 0.]],\n",
      "\n",
      "          [[0., 0., 0.],\n",
      "           [0., 0., 0.],\n",
      "           [0., 0., 0.]],\n",
      "\n",
      "          [[0., 0., 0.],\n",
      "           [0., 0., 0.],\n",
      "           [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "         [[[0., 0., 0.],\n",
      "           [0., 0., 0.],\n",
      "           [0., 0., 0.]],\n",
      "\n",
      "          [[0., 0., 0.],\n",
      "           [0., 0., 0.],\n",
      "           [0., 0., 0.]],\n",
      "\n",
      "          [[0., 0., 0.],\n",
      "           [0., 0., 0.],\n",
      "           [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "         [[[0., 0., 0.],\n",
      "           [0., 0., 0.],\n",
      "           [0., 0., 0.]],\n",
      "\n",
      "          [[0., 0., 0.],\n",
      "           [0., 0., 0.],\n",
      "           [0., 0., 0.]],\n",
      "\n",
      "          [[0., 0., 0.],\n",
      "           [0., 0., 0.],\n",
      "           [0., 0., 0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0., 0., 0.],\n",
      "           [0., 0., 0.],\n",
      "           [0., 0., 0.]],\n",
      "\n",
      "          [[0., 0., 0.],\n",
      "           [0., 0., 0.],\n",
      "           [0., 0., 0.]],\n",
      "\n",
      "          [[0., 0., 0.],\n",
      "           [0., 0., 0.],\n",
      "           [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "         [[[0., 0., 0.],\n",
      "           [0., 0., 0.],\n",
      "           [0., 0., 0.]],\n",
      "\n",
      "          [[0., 0., 0.],\n",
      "           [0., 0., 0.],\n",
      "           [0., 0., 0.]],\n",
      "\n",
      "          [[0., 0., 0.],\n",
      "           [0., 0., 0.],\n",
      "           [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "         [[[0., 0., 0.],\n",
      "           [0., 0., 0.],\n",
      "           [0., 0., 0.]],\n",
      "\n",
      "          [[0., 0., 0.],\n",
      "           [0., 0., 0.],\n",
      "           [0., 0., 0.]],\n",
      "\n",
      "          [[0., 0., 0.],\n",
      "           [0., 0., 0.],\n",
      "           [0., 0., 0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0., 0., 0.],\n",
      "           [0., 0., 0.],\n",
      "           [0., 0., 0.]],\n",
      "\n",
      "          [[0., 0., 0.],\n",
      "           [0., 0., 0.],\n",
      "           [0., 0., 0.]],\n",
      "\n",
      "          [[0., 0., 0.],\n",
      "           [0., 0., 0.],\n",
      "           [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "         [[[0., 0., 0.],\n",
      "           [0., 0., 0.],\n",
      "           [0., 0., 0.]],\n",
      "\n",
      "          [[0., 0., 0.],\n",
      "           [0., 0., 0.],\n",
      "           [0., 0., 0.]],\n",
      "\n",
      "          [[0., 0., 0.],\n",
      "           [0., 0., 0.],\n",
      "           [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "         [[[0., 0., 0.],\n",
      "           [0., 0., 0.],\n",
      "           [0., 0., 0.]],\n",
      "\n",
      "          [[0., 0., 0.],\n",
      "           [0., 0., 0.],\n",
      "           [0., 0., 0.]],\n",
      "\n",
      "          [[0., 0., 0.],\n",
      "           [0., 0., 0.],\n",
      "           [0., 0., 0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0., 0., 0.],\n",
      "           [0., 0., 0.],\n",
      "           [0., 0., 0.]],\n",
      "\n",
      "          [[0., 0., 0.],\n",
      "           [0., 0., 0.],\n",
      "           [0., 0., 0.]],\n",
      "\n",
      "          [[0., 0., 0.],\n",
      "           [0., 0., 0.],\n",
      "           [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "         [[[0., 0., 0.],\n",
      "           [0., 0., 0.],\n",
      "           [0., 0., 0.]],\n",
      "\n",
      "          [[0., 0., 0.],\n",
      "           [0., 0., 0.],\n",
      "           [0., 0., 0.]],\n",
      "\n",
      "          [[0., 0., 0.],\n",
      "           [0., 0., 0.],\n",
      "           [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "         [[[0., 0., 0.],\n",
      "           [0., 0., 0.],\n",
      "           [0., 0., 0.]],\n",
      "\n",
      "          [[0., 0., 0.],\n",
      "           [0., 0., 0.],\n",
      "           [0., 0., 0.]],\n",
      "\n",
      "          [[0., 0., 0.],\n",
      "           [0., 0., 0.],\n",
      "           [0., 0., 0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0., 0., 0.],\n",
      "           [0., 0., 0.],\n",
      "           [0., 0., 0.]],\n",
      "\n",
      "          [[0., 0., 0.],\n",
      "           [0., 0., 0.],\n",
      "           [0., 0., 0.]],\n",
      "\n",
      "          [[0., 0., 0.],\n",
      "           [0., 0., 0.],\n",
      "           [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "         [[[0., 0., 0.],\n",
      "           [0., 0., 0.],\n",
      "           [0., 0., 0.]],\n",
      "\n",
      "          [[0., 0., 0.],\n",
      "           [0., 0., 0.],\n",
      "           [0., 0., 0.]],\n",
      "\n",
      "          [[0., 0., 0.],\n",
      "           [0., 0., 0.],\n",
      "           [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "         [[[0., 0., 0.],\n",
      "           [0., 0., 0.],\n",
      "           [0., 0., 0.]],\n",
      "\n",
      "          [[0., 0., 0.],\n",
      "           [0., 0., 0.],\n",
      "           [0., 0., 0.]],\n",
      "\n",
      "          [[0., 0., 0.],\n",
      "           [0., 0., 0.],\n",
      "           [0., 0., 0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0., 0., 0.],\n",
      "           [0., 0., 0.],\n",
      "           [0., 0., 0.]],\n",
      "\n",
      "          [[0., 0., 0.],\n",
      "           [0., 0., 0.],\n",
      "           [0., 0., 0.]],\n",
      "\n",
      "          [[0., 0., 0.],\n",
      "           [0., 0., 0.],\n",
      "           [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "         [[[0., 0., 0.],\n",
      "           [0., 0., 0.],\n",
      "           [0., 0., 0.]],\n",
      "\n",
      "          [[0., 0., 0.],\n",
      "           [0., 0., 0.],\n",
      "           [0., 0., 0.]],\n",
      "\n",
      "          [[0., 0., 0.],\n",
      "           [0., 0., 0.],\n",
      "           [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "         [[[0., 0., 0.],\n",
      "           [0., 0., 0.],\n",
      "           [0., 0., 0.]],\n",
      "\n",
      "          [[0., 0., 0.],\n",
      "           [0., 0., 0.],\n",
      "           [0., 0., 0.]],\n",
      "\n",
      "          [[0., 0., 0.],\n",
      "           [0., 0., 0.],\n",
      "           [0., 0., 0.]]]]])\n",
      "Fan-In: 81\n",
      "Fan-Out: 432\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn \n",
    "\n",
    "# (out_channels, in_channels, depth, height, width)\n",
    "weight = torch.empty(16, 3, 3, 3, 3)\n",
    "print(\"weight\", weight)\n",
    "\n",
    "fan_in, fan_out = nn.init._calculate_fan_in_and_fan_out(weight)\n",
    "print(\"Fan-In:\", fan_in)\n",
    "print(\"Fan-Out:\", fan_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 3, 3, 3])\n",
      "torch.Size([1, 16, 30, 30, 30])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a 3D convolutional layer\n",
    "conv3d_layer = nn.Conv3d(in_channels=3, out_channels=16, kernel_size=(3, 3, 3))\n",
    "\n",
    "# Print the shape of the weight tensor\n",
    "print(conv3d_layer.weight.shape)  # Should output torch.Size([16, 3, 3, 3, 3])\n",
    "\n",
    "# Example input tensor (e.g., a batch of 3D volumes)\n",
    "input_tensor = torch.randn(1, 3, 32, 32, 32)  # shape (batch_size, in_channels, depth, height, width)\n",
    "\n",
    "# Apply the convolutional layer to the input tensor\n",
    "output_tensor = conv3d_layer(input_tensor)\n",
    "\n",
    "# Print the shape of the output tensor\n",
    "print(output_tensor.shape)  # Output shape depends on stride, padding, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## how to work inspect.signature()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter name: x\n",
      "  Default value: <class 'inspect._empty'>\n",
      "  Annotation: <class 'inspect._empty'>\n",
      "  Kind: POSITIONAL_OR_KEYWORD\n",
      "Parameter name: y\n",
      "  Default value: <class 'inspect._empty'>\n",
      "  Annotation: <class 'inspect._empty'>\n",
      "  Kind: POSITIONAL_OR_KEYWORD\n",
      "Parameter name: z\n",
      "  Default value: 0\n",
      "  Annotation: <class 'inspect._empty'>\n",
      "  Kind: POSITIONAL_OR_KEYWORD\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "import torch.nn as nn\n",
    "\n",
    "class ExampleDecoder(nn.Module):\n",
    "    def forward(self, x, y, z=0):\n",
    "        pass\n",
    "\n",
    "# Create an instance of the ExampleDecoder\n",
    "decoder = ExampleDecoder()\n",
    "\n",
    "# Get the signature of the forward method\n",
    "signature = inspect.signature(decoder.forward)\n",
    "\n",
    "# Get the parameters of the forward method\n",
    "parameters = signature.parameters\n",
    "\n",
    "# Print the parameters\n",
    "for name, param in parameters.items():\n",
    "    print(f\"Parameter name: {name}\")\n",
    "    print(f\"  Default value: {param.default}\")\n",
    "    print(f\"  Annotation: {param.annotation}\")\n",
    "    print(f\"  Kind: {param.kind}\")\n",
    "\n",
    "# This will output:\n",
    "# Parameter name: x\n",
    "#   Default value: <class 'inspect._empty'>\n",
    "#   Annotation: <class 'inspect._empty'>\n",
    "#   Kind: POSITIONAL_OR_KEYWORD\n",
    "# Parameter name: y\n",
    "#   Default value: <class 'inspect._empty'>\n",
    "#   Annotation: <class 'inspect._empty'>\n",
    "#   Kind: POSITIONAL_OR_KEYWORD\n",
    "# Parameter name: z\n",
    "#   Default value: 0\n",
    "#   Annotation: <class 'inspect._empty'>\n",
    "#   Kind: POSITIONAL_OR_KEYWORD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConfigMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mandhata Kumar\\AppData\\Local\\Temp\\ipykernel_9580\\3783680460.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(f\"{load_directory}/pytorch_model.bin\")\n"
     ]
    }
   ],
   "source": [
    "from diffusers import ModelMixin, ConfigMixin\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class ExampleModel(ConfigMixin, nn.Module):\n",
    "    config_name = \"example_model\"\n",
    "\n",
    "    def __init__(self, param1: int = 1, param2: str = \"default\"):\n",
    "        super().__init__()\n",
    "        self.param1 = param1\n",
    "        self.param2 = param2\n",
    "        self.layer = nn.Linear(10, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "\n",
    "    def save_pretrained(self, save_directory):\n",
    "        # Save the model's state_dict (weights)\n",
    "        torch.save(self.state_dict(), f\"{save_directory}/pytorch_model.bin\")\n",
    "        # Optionally save the configuration\n",
    "        self.save_config(save_directory)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, load_directory):\n",
    "        # Load the model's state_dict (weights)\n",
    "        state_dict = torch.load(f\"{load_directory}/pytorch_model.bin\")\n",
    "        # Instantiate the model\n",
    "        model = cls()\n",
    "        # Load the state_dict into the model\n",
    "        model.load_state_dict(state_dict)\n",
    "        return model\n",
    "\n",
    "# Example usage to save and load the model\n",
    "model = ExampleModel(param1=42, param2=\"example\")\n",
    "model.save_pretrained(\"D:\\\\AI\\\\test\\\\config_mixin\")\n",
    "\n",
    "# Loading the model\n",
    "loaded_model = ExampleModel.from_pretrained(\"D:\\\\AI\\\\test\\\\config_mixin\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Union\n",
    "\n",
    "import torch\n",
    "import inspect\n",
    "import math\n",
    "import torch.nn as nn\n",
    "from diffusers import ConfigMixin, ModelMixin\n",
    "from diffusers.models.autoencoders.vae import (\n",
    "    DecoderOutput,\n",
    "    DiagonalGaussianDistribution,\n",
    ")\n",
    "from diffusers.models.modeling_outputs import AutoencoderKLOutput\n",
    "from ltx_video.models.autoencoders.conv_nd_factory import make_conv_nd\n",
    "\n",
    "\n",
    "class AutoencoderKLWrapper(ModelMixin, ConfigMixin):\n",
    "    \"\"\"Variational Autoencoder (VAE) model with KL loss.\n",
    "\n",
    "    VAE from the paper Auto-Encoding Variational Bayes by Diederik P. Kingma and Max Welling.\n",
    "    This model is a wrapper around an encoder and a decoder, and it adds a KL loss term to the reconstruction loss.\n",
    "\n",
    "    Args:\n",
    "        encoder (`nn.Module`):\n",
    "            Encoder module.\n",
    "        decoder (`nn.Module`):\n",
    "            Decoder module.\n",
    "        latent_channels (`int`, *optional*, defaults to 4):\n",
    "            Number of latent channels.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder: nn.Module,\n",
    "        decoder: nn.Module,\n",
    "        latent_channels: int = 4,\n",
    "        dims: int = 2,\n",
    "        sample_size=512,\n",
    "        use_quant_conv: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # pass init params to Encoder\n",
    "        self.encoder = encoder\n",
    "        self.use_quant_conv = use_quant_conv\n",
    "\n",
    "        # pass init params to Decoder\n",
    "        quant_dims = 2 if dims == 2 else 3\n",
    "        self.decoder = decoder\n",
    "        if use_quant_conv:\n",
    "            self.quant_conv = make_conv_nd(\n",
    "                quant_dims, 2 * latent_channels, 2 * latent_channels, 1\n",
    "            )\n",
    "            self.post_quant_conv = make_conv_nd(\n",
    "                quant_dims, latent_channels, latent_channels, 1\n",
    "            )\n",
    "        else:\n",
    "            self.quant_conv = nn.Identity()\n",
    "            self.post_quant_conv = nn.Identity()\n",
    "        self.use_z_tiling = False\n",
    "        self.use_hw_tiling = False\n",
    "        self.dims = dims\n",
    "        self.z_sample_size = 1\n",
    "\n",
    "        self.decoder_params = inspect.signature(self.decoder.forward).parameters\n",
    "\n",
    "        # only relevant if vae tiling is enabled\n",
    "        self.set_tiling_params(sample_size=sample_size, overlap_factor=0.25)\n",
    "\n",
    "    def set_tiling_params(self, sample_size: int = 512, overlap_factor: float = 0.25):\n",
    "        self.tile_sample_min_size = sample_size\n",
    "        num_blocks = len(self.encoder.down_blocks)\n",
    "        self.tile_latent_min_size = int(sample_size / (2 ** (num_blocks - 1)))\n",
    "        self.tile_overlap_factor = overlap_factor\n",
    "\n",
    "    def enable_z_tiling(self, z_sample_size: int = 8):\n",
    "        r\"\"\"\n",
    "        Enable tiling during VAE decoding.\n",
    "\n",
    "        When this option is enabled, the VAE will split the input tensor in tiles to compute decoding in several\n",
    "        steps. This is useful to save some memory and allow larger batch sizes.\n",
    "        \"\"\"\n",
    "        self.use_z_tiling = z_sample_size > 1\n",
    "        self.z_sample_size = z_sample_size\n",
    "        assert (\n",
    "            z_sample_size % 8 == 0 or z_sample_size == 1\n",
    "        ), f\"z_sample_size must be a multiple of 8 or 1. Got {z_sample_size}.\"\n",
    "\n",
    "    def disable_z_tiling(self):\n",
    "        r\"\"\"\n",
    "        Disable tiling during VAE decoding. If `use_tiling` was previously invoked, this method will go back to computing\n",
    "        decoding in one step.\n",
    "        \"\"\"\n",
    "        self.use_z_tiling = False\n",
    "\n",
    "    def enable_hw_tiling(self):\n",
    "        r\"\"\"\n",
    "        Enable tiling during VAE decoding along the height and width dimension.\n",
    "        \"\"\"\n",
    "        self.use_hw_tiling = True\n",
    "\n",
    "    def disable_hw_tiling(self):\n",
    "        r\"\"\"\n",
    "        Disable tiling during VAE decoding along the height and width dimension.\n",
    "        \"\"\"\n",
    "        self.use_hw_tiling = False\n",
    "\n",
    "    def _hw_tiled_encode(self, x: torch.FloatTensor, return_dict: bool = True):\n",
    "        overlap_size = int(self.tile_sample_min_size * (1 - self.tile_overlap_factor))\n",
    "        blend_extent = int(self.tile_latent_min_size * self.tile_overlap_factor)\n",
    "        row_limit = self.tile_latent_min_size - blend_extent\n",
    "\n",
    "        # Split the image into 512x512 tiles and encode them separately.\n",
    "        rows = []\n",
    "        for i in range(0, x.shape[3], overlap_size):\n",
    "            row = []\n",
    "            for j in range(0, x.shape[4], overlap_size):\n",
    "                tile = x[\n",
    "                    :,\n",
    "                    :,\n",
    "                    :,\n",
    "                    i : i + self.tile_sample_min_size,\n",
    "                    j : j + self.tile_sample_min_size,\n",
    "                ]\n",
    "                tile = self.encoder(tile)\n",
    "                tile = self.quant_conv(tile)\n",
    "                row.append(tile)\n",
    "            rows.append(row)\n",
    "        result_rows = []\n",
    "        for i, row in enumerate(rows):\n",
    "            result_row = []\n",
    "            for j, tile in enumerate(row):\n",
    "                # blend the above tile and the left tile\n",
    "                # to the current tile and add the current tile to the result row\n",
    "                if i > 0:\n",
    "                    tile = self.blend_v(rows[i - 1][j], tile, blend_extent)\n",
    "                if j > 0:\n",
    "                    tile = self.blend_h(row[j - 1], tile, blend_extent)\n",
    "                result_row.append(tile[:, :, :, :row_limit, :row_limit])\n",
    "            result_rows.append(torch.cat(result_row, dim=4))\n",
    "\n",
    "        moments = torch.cat(result_rows, dim=3)\n",
    "        return moments\n",
    "\n",
    "    def blend_z(\n",
    "        self, a: torch.Tensor, b: torch.Tensor, blend_extent: int\n",
    "    ) -> torch.Tensor:\n",
    "        blend_extent = min(a.shape[2], b.shape[2], blend_extent)\n",
    "        for z in range(blend_extent):\n",
    "            b[:, :, z, :, :] = a[:, :, -blend_extent + z, :, :] * (\n",
    "                1 - z / blend_extent\n",
    "            ) + b[:, :, z, :, :] * (z / blend_extent)\n",
    "        return b\n",
    "\n",
    "    def blend_v(\n",
    "        self, a: torch.Tensor, b: torch.Tensor, blend_extent: int\n",
    "    ) -> torch.Tensor:\n",
    "        blend_extent = min(a.shape[3], b.shape[3], blend_extent)\n",
    "        for y in range(blend_extent):\n",
    "            b[:, :, :, y, :] = a[:, :, :, -blend_extent + y, :] * (\n",
    "                1 - y / blend_extent\n",
    "            ) + b[:, :, :, y, :] * (y / blend_extent)\n",
    "        return b\n",
    "\n",
    "    def blend_h(\n",
    "        self, a: torch.Tensor, b: torch.Tensor, blend_extent: int\n",
    "    ) -> torch.Tensor:\n",
    "        blend_extent = min(a.shape[4], b.shape[4], blend_extent)\n",
    "        for x in range(blend_extent):\n",
    "            b[:, :, :, :, x] = a[:, :, :, :, -blend_extent + x] * (\n",
    "                1 - x / blend_extent\n",
    "            ) + b[:, :, :, :, x] * (x / blend_extent)\n",
    "        return b\n",
    "\n",
    "    def _hw_tiled_decode(self, z: torch.FloatTensor, target_shape):\n",
    "        overlap_size = int(self.tile_latent_min_size * (1 - self.tile_overlap_factor))\n",
    "        blend_extent = int(self.tile_sample_min_size * self.tile_overlap_factor)\n",
    "        row_limit = self.tile_sample_min_size - blend_extent\n",
    "        tile_target_shape = (\n",
    "            *target_shape[:3],\n",
    "            self.tile_sample_min_size,\n",
    "            self.tile_sample_min_size,\n",
    "        )\n",
    "        # Split z into overlapping 64x64 tiles and decode them separately.\n",
    "        # The tiles have an overlap to avoid seams between tiles.\n",
    "        rows = []\n",
    "        for i in range(0, z.shape[3], overlap_size):\n",
    "            row = []\n",
    "            for j in range(0, z.shape[4], overlap_size):\n",
    "                tile = z[\n",
    "                    :,\n",
    "                    :,\n",
    "                    :,\n",
    "                    i : i + self.tile_latent_min_size,\n",
    "                    j : j + self.tile_latent_min_size,\n",
    "                ]\n",
    "                tile = self.post_quant_conv(tile)\n",
    "                decoded = self.decoder(tile, target_shape=tile_target_shape)\n",
    "                row.append(decoded)\n",
    "            rows.append(row)\n",
    "        result_rows = []\n",
    "        for i, row in enumerate(rows):\n",
    "            result_row = []\n",
    "            for j, tile in enumerate(row):\n",
    "                # blend the above tile and the left tile\n",
    "                # to the current tile and add the current tile to the result row\n",
    "                if i > 0:\n",
    "                    tile = self.blend_v(rows[i - 1][j], tile, blend_extent)\n",
    "                if j > 0:\n",
    "                    tile = self.blend_h(row[j - 1], tile, blend_extent)\n",
    "                result_row.append(tile[:, :, :, :row_limit, :row_limit])\n",
    "            result_rows.append(torch.cat(result_row, dim=4))\n",
    "\n",
    "        dec = torch.cat(result_rows, dim=3)\n",
    "        return dec\n",
    "\n",
    "    def encode(\n",
    "        self, z: torch.FloatTensor, return_dict: bool = True\n",
    "    ) -> Union[DecoderOutput, torch.FloatTensor]:\n",
    "        if self.use_z_tiling and z.shape[2] > self.z_sample_size > 1:\n",
    "            num_splits = z.shape[2] // self.z_sample_size\n",
    "            sizes = [self.z_sample_size] * num_splits\n",
    "            sizes = (\n",
    "                sizes + [z.shape[2] - sum(sizes)]\n",
    "                if z.shape[2] - sum(sizes) > 0\n",
    "                else sizes\n",
    "            )\n",
    "            tiles = z.split(sizes, dim=2)\n",
    "            moments_tiles = [\n",
    "                (\n",
    "                    self._hw_tiled_encode(z_tile, return_dict)\n",
    "                    if self.use_hw_tiling\n",
    "                    else self._encode(z_tile)\n",
    "                )\n",
    "                for z_tile in tiles\n",
    "            ]\n",
    "            moments = torch.cat(moments_tiles, dim=2)\n",
    "\n",
    "        else:\n",
    "            moments = (\n",
    "                self._hw_tiled_encode(z, return_dict)\n",
    "                if self.use_hw_tiling\n",
    "                else self._encode(z)\n",
    "            )\n",
    "\n",
    "        posterior = DiagonalGaussianDistribution(moments)\n",
    "        if not return_dict:\n",
    "            return (posterior,)\n",
    "\n",
    "        return AutoencoderKLOutput(latent_dist=posterior)\n",
    "\n",
    "    def _encode(self, x: torch.FloatTensor) -> AutoencoderKLOutput:\n",
    "        h = self.encoder(x)\n",
    "        moments = self.quant_conv(h)\n",
    "        return moments\n",
    "\n",
    "    def _decode(\n",
    "        self,\n",
    "        z: torch.FloatTensor,\n",
    "        target_shape=None,\n",
    "        timesteps: Optional[torch.Tensor] = None,\n",
    "    ) -> Union[DecoderOutput, torch.FloatTensor]:\n",
    "        z = self.post_quant_conv(z)\n",
    "        if \"timesteps\" in self.decoder_params:\n",
    "            dec = self.decoder(z, target_shape=target_shape, timesteps=timesteps)\n",
    "        else:\n",
    "            dec = self.decoder(z, target_shape=target_shape)\n",
    "        return dec\n",
    "\n",
    "    def decode(\n",
    "        self,\n",
    "        z: torch.FloatTensor,\n",
    "        return_dict: bool = True,\n",
    "        target_shape=None,\n",
    "        timesteps: Optional[torch.Tensor] = None,\n",
    "    ) -> Union[DecoderOutput, torch.FloatTensor]:\n",
    "        assert target_shape is not None, \"target_shape must be provided for decoding\"\n",
    "        if self.use_z_tiling and z.shape[2] > self.z_sample_size > 1:\n",
    "            reduction_factor = int(\n",
    "                self.encoder.patch_size_t\n",
    "                * 2\n",
    "                ** (\n",
    "                    len(self.encoder.down_blocks)\n",
    "                    - 1\n",
    "                    - math.sqrt(self.encoder.patch_size)\n",
    "                )\n",
    "            )\n",
    "            split_size = self.z_sample_size // reduction_factor\n",
    "            num_splits = z.shape[2] // split_size\n",
    "\n",
    "            # copy target shape, and divide frame dimension (=2) by the context size\n",
    "            target_shape_split = list(target_shape)\n",
    "            target_shape_split[2] = target_shape[2] // num_splits\n",
    "\n",
    "            decoded_tiles = [\n",
    "                (\n",
    "                    self._hw_tiled_decode(z_tile, target_shape_split)\n",
    "                    if self.use_hw_tiling\n",
    "                    else self._decode(z_tile, target_shape=target_shape_split)\n",
    "                )\n",
    "                for z_tile in torch.tensor_split(z, num_splits, dim=2)\n",
    "            ]\n",
    "            decoded = torch.cat(decoded_tiles, dim=2)\n",
    "        else:\n",
    "            decoded = (\n",
    "                self._hw_tiled_decode(z, target_shape)\n",
    "                if self.use_hw_tiling\n",
    "                else self._decode(z, target_shape=target_shape, timesteps=timesteps)\n",
    "            )\n",
    "\n",
    "        if not return_dict:\n",
    "            return (decoded,)\n",
    "\n",
    "        return DecoderOutput(sample=decoded)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        sample: torch.FloatTensor,\n",
    "        sample_posterior: bool = False,\n",
    "        return_dict: bool = True,\n",
    "        generator: Optional[torch.Generator] = None,\n",
    "    ) -> Union[DecoderOutput, torch.FloatTensor]:\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            sample (`torch.FloatTensor`): Input sample.\n",
    "            sample_posterior (`bool`, *optional*, defaults to `False`):\n",
    "                Whether to sample from the posterior.\n",
    "            return_dict (`bool`, *optional*, defaults to `True`):\n",
    "                Whether to return a [`DecoderOutput`] instead of a plain tuple.\n",
    "            generator (`torch.Generator`, *optional*):\n",
    "                Generator used to sample from the posterior.\n",
    "        \"\"\"\n",
    "        x = sample\n",
    "        posterior = self.encode(x).latent_dist\n",
    "        if sample_posterior:\n",
    "            z = posterior.sample(generator=generator)\n",
    "        else:\n",
    "            z = posterior.mode()\n",
    "        dec = self.decode(z, target_shape=sample.shape).sample\n",
    "\n",
    "        if not return_dict:\n",
    "            return (dec,)\n",
    "\n",
    "        return DecoderOutput(sample=dec)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
